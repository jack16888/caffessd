Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		34.504		0.898		0.898		0.495		0.093
conv1_2		51.192		0.674		0.606		0.153		0.044
conv2_1		41.603		1.230		0.745		0.296		0.027
conv2_2		48.787		0.853		0.635		0.116		0.018
conv3_1		34.203		1.426		0.906		0.157		0.018
conv3_2		34.183		1.001		0.907		0.097		0.027
conv3_3		29.019		1.178		1.068		0.133		0.048
conv4_1		22.289		1.302		1.391		0.121		0.041
conv4_2		16.420		1.357		1.888		0.105		0.067
conv4_3		9.480		1.732		3.270		0.172		0.253
conv5_1		7.712		1.229		4.020		0.111		0.633
conv5_2		5.293		1.457		5.857		0.146		1.117
conv5_3		3.102		1.707		9.995		0.233		0.043
scale factor:   0.100064516129

Refreshing network prototxt and caffemodel ...

