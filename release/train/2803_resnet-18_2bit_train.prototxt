name: "ResNet-18"
layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
        phase: TRAIN
    }
    transform_param {
        mirror: true
        crop_size: 224
        mean_value: 104
        mean_value: 117
        mean_value: 123
    }
    data_param {
    source: "path_to_lmdb"
    batch_size: 32 
    backend: LMDB
    }
}
layer {
    name: "data"
    type: "Data"
    top: "data"
    top: "label"
    include {
        phase: TEST
    }
    transform_param {
        mirror: false
        crop_size: 224
        mean_value: 104
        mean_value: 117
        mean_value: 123
    }
    data_param {
    source: "path_to_lmdb"
    batch_size: 10 
    backend: LMDB
    }
}
layer {
    bottom: "data"
    top: "conv1a"
    name: "conv1a"
    type: "QuantConvolution"
    param {
        lr_mult: 1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "conv1a"
    top: "conv1a"
    name: "bn_conv1a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
}

layer {
    bottom: "conv1a"
    top: "conv1a"
    name: "scale_conv1a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv1a"
    top: "conv1a"
    name: "conv1a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.826 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "conv1a"
    top: "conv1b"
    name: "conv1b"
    type: "QuantConvolution"
    param {
        lr_mult: 1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "conv1b"
    top: "conv1b"
    name: "bn_conv1b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
}

layer {
    bottom: "conv1b"
    top: "conv1b"
    name: "scale_conv1b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv1b"
    top: "conv1b"
    name: "conv1b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.85
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "conv1b"
    top: "pool0"
    name: "pool0"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}


layer {
    bottom: "pool0"
    top: "conv2a"
    name: "conv2a"
    type: "QuantConvolution"
    param {
        lr_mult: 1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "conv2a"
    top: "conv2a"
    name: "bn_conv2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
}

layer {
    bottom: "conv2a"
    top: "conv2a"
    name: "scale_conv2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv2a"
    top: "conv2a"
    name: "conv2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.062
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "conv2a"
    top: "conv2b"
    name: "conv2b"
    type: "QuantConvolution"
    param {
        lr_mult: 1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "conv2b"
    top: "conv2b"
    name: "bn_conv2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
}

layer {
    bottom: "conv2b"
    top: "conv2b"
    name: "scale_conv2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "conv2b"
    top: "conv2b"
    name: "conv2b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.924
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "conv2b"
    top: "pool1"
    name: "pool1"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}

layer {
    bottom: "pool1"
    top: "res2a_branch2a"
    name: "res2a_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "bn2a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "scale2a_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res2a_branch2a"
    top: "res2a_branch2a"
    name: "res2a_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.904
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res2a_branch2a"
    top: "res2a_branch2b"
    name: "res2a_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res2a_branch2b"
    top: "res2a_branch2b"
    name: "bn2a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res2a_branch2b"
    top: "res2a_branch2b"
    name: "scale2a_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "pool1"
    bottom: "res2a_branch2b"
    top: "res2a"
    name: "res2a"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "res2a"
    top: "res2a"
    name: "res2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.046
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res2a"
    top: "res2b_branch2a"
    name: "res2b_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "bn2b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "scale2b_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res2b_branch2a"
    top: "res2b_branch2a"
    name: "res2b_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.944
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res2b_branch2a"
    top: "res2b_branch2b"
    name: "res2b_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res2b_branch2b"
    top: "res2b_branch2b"
    name: "bn2b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res2b_branch2b"
    top: "res2b_branch2b"
    name: "scale2b_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res2a"
    bottom: "res2b_branch2b"
    top: "res2b"
    name: "res2b"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "res2b"
    top: "res2b"
    name: "res2b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.086
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}
layer {
    bottom: "res2b"
    top: "pool2"
    name: "pool2"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}


layer {
    bottom: "pool2"
    top: "res3a_branch2a"
    name: "res3a_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "bn3a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "scale3a_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res3a_branch2a"
    top: "res3a_branch2a"
    name: "res3a_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.955
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res3a_branch2a"
    top: "res3a_branch2b"
    name: "res3a_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res3a_branch2b"
    top: "res3a_branch2b"
    name: "bn3a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res3a_branch2b"
    top: "res3a_branch2b"
    name: "scale3a_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res3a_branch2b"
    top: "res3a_branch2b"
    name: "res3a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.212
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res3a_branch2b"
    top: "res3b_branch2a"
    name: "res3b_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "bn3b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "scale3b_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res3b_branch2a"
    top: "res3b_branch2a"
    name: "res3b_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.245
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res3b_branch2a"
    top: "res3b_branch2b"
    name: "res3b_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 128
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res3b_branch2b"
    top: "res3b_branch2b"
    name: "bn3b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res3b_branch2b"
    top: "res3b_branch2b"
    name: "scale3b_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res3a_branch2b"
    bottom: "res3b_branch2b"
    top: "res3b"
    name: "res3b"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "res3b"
    top: "res3b"
    name: "res3b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.941
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res3b"
    top: "pool3"
    name: "pool3"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}

layer {
    bottom: "pool3"
    top: "res4a_branch2a"
    name: "res4a_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "bn4a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "scale4a_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res4a_branch2a"
    top: "res4a_branch2a"
    name: "res4a_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.999
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res4a_branch2a"
    top: "res4a_branch2b"
    name: "res4a_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res4a_branch2b"
    top: "res4a_branch2b"
    name: "bn4a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res4a_branch2b"
    top: "res4a_branch2b"
    name: "scale4a_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res4a_branch2b"
    top: "res4a_branch2b"
    name: "res4a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.904
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res4a_branch2b"
    top: "res4b_branch2a"
    name: "res4b_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "bn4b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "scale4b_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res4b_branch2a"
    top: "res4b_branch2a"
    name: "res4b_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.869 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res4b_branch2a"
    top: "res4b_branch2b"
    name: "res4b_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 256
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res4b_branch2b"
    top: "res4b_branch2b"
    name: "bn4b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res4b_branch2b"
    top: "res4b_branch2b"
    name: "scale4b_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res4a_branch2b"
    bottom: "res4b_branch2b"
    top: "res4b"
    name: "res4b"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "res4b"
    top: "res4b"
    name: "res4b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.930 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res4b"
    top: "pool4"
    name: "pool4"
    type: "Pooling"
    pooling_param {
        kernel_size: 2
        stride: 2
        pool: MAX
    }
}

layer {
    bottom: "pool4"
    top: "res5a_branch2a"
    name: "res5a_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res5a_branch2a"
    top: "res5a_branch2a"
    name: "bn5a_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res5a_branch2a"
    top: "res5a_branch2a"
    name: "scale5a_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res5a_branch2a"
    top: "res5a_branch2a"
    name: "res5a_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 0.989 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res5a_branch2a"
    top: "res5a_branch2b"
    name: "res5a_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult: 1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res5a_branch2b"
    top: "res5a_branch2b"
    name: "bn5a_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res5a_branch2b"
    top: "res5a_branch2b"
    name: "scale5a_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res5a_branch2b"
    top: "res5a_branch2b"
    name: "res5a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.448 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res5a_branch2b"
    top: "res5b_branch2a"
    name: "res5b_branch2a"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res5b_branch2a"
    top: "res5b_branch2a"
    name: "bn5b_branch2a"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res5b_branch2a"
    top: "res5b_branch2a"
    name: "scale5b_branch2a"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res5b_branch2a"
    top: "res5b_branch2a"
    name: "res5b_branch2a_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 1.449 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res5b_branch2a"
    top: "res5b_branch2b"
    name: "res5b_branch2b"
    type: "QuantConvolution"
    param {
        lr_mult:1.0
        decay_mult: 1.0
    }
    convolution_param {
        num_output: 512
        kernel_size: 3
        pad: 1
        stride: 1
        weight_filler {
            type: "msra"
        }
        bias_term: false

    }
    quant_convolution_param {
      coef_precision: TWO_BITS
      bw_params: 12
      shift_enable: false
    }
}

layer {
    bottom: "res5b_branch2b"
    top: "res5b_branch2b"
    name: "bn5b_branch2b"
    type: "BatchNorm"
    batch_norm_param {
      moving_average_fraction: 0.9 
    }
    
}

layer {
    bottom: "res5b_branch2b"
    top: "res5b_branch2b"
    name: "scale5b_branch2b"
    type: "Scale"
    scale_param {
        bias_term: true
    }
}

layer {
    bottom: "res5a_branch2b"
    bottom: "res5b_branch2b"
    top: "res5b"
    name: "res5b"
    type: "Eltwise"
    eltwise_param {
        operation: SUM
    }
}

layer {
    bottom: "res5b"
    top: "res5b"
    name: "res5b_relu"
    type: "QuantReLU"
    quant_relu_param {
      filler {
        type: "constant"
        value: 6.637 
      }
      channel_shared: true
      act_bits: 5
      quant_enable: false
    }
    param {lr_mult: 0.0}
}

layer {
    bottom: "res5b"
    top: "pool5"
    name: "pool5"
    type: "Pooling"
    pooling_param {
        kernel_size: 7
        stride: 1
        pool: AVE
    }
}

layer {
    bottom: "pool5"
    top: "fc1000"
    name: "fc1000"
    type: "InnerProduct"
    param {
        lr_mult: 1
        decay_mult: 1
    }
    param {
        lr_mult: 2
        decay_mult: 1
    }
    inner_product_param {
        num_output: 1000
        weight_filler {
            type: "xavier"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    bottom: "fc1000"
    bottom: "label"
    name: "loss"
    type: "SoftmaxWithLoss"
    top: "loss"
}

layer {
    bottom: "fc1000"
    bottom: "label"
    top: "acc/top-1"
    name: "acc/top-1"
    type: "Accuracy"
    include {
        phase: TEST
    }
}

layer {
    bottom: "fc1000"
    bottom: "label"
    top: "acc/top-5"
    name: "acc/top-5"
    type: "Accuracy"
    include {
        phase: TEST
    }
    accuracy_param {
        top_k: 5
    }
}
