Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		34.504		0.898		0.898		0.495		0.093
conv1_2		51.192		0.674		0.606		0.153		0.044
conv2_1		41.603		1.230		0.745		0.296		0.027
conv2_2		48.787		0.853		0.635		0.116		0.018
conv3_1		34.203		1.426		0.906		0.157		0.018
conv3_2		34.183		1.001		0.907		0.097		0.027
conv3_3		29.019		1.178		1.068		0.133		0.048
conv4_1		22.289		1.302		1.391		0.121		0.041
conv4_2		16.420		1.357		1.888		0.105		0.067
conv4_3		9.480		1.732		3.270		0.172		0.253
conv5_1		7.712		1.229		4.020		0.111		0.633
conv5_2		5.293		1.457		5.857		0.146		1.117
conv5_3		3.102		1.707		9.995		0.233		0.043
scale factor:   0.100064516129

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		25.086		1.236		1.236		0.549		0.200
conv1_2		34.234		0.733		0.906		0.212		0.135
conv2_1		29.387		1.165		1.055		0.231		0.063
conv2_2		34.185		0.860		0.907		0.119		0.040
conv3_1		27.895		1.225		1.111		0.135		0.040
conv3_2		28.723		0.971		1.079		0.101		0.035
conv3_3		27.010		1.063		1.148		0.120		0.033
conv4_1		21.086		1.281		1.470		0.124		0.039
conv4_2		14.764		1.428		2.100		0.115		0.057
conv4_3		7.611		1.940		4.073		0.176		0.187
conv5_1		5.922		1.285		5.235		0.182		0.454
conv5_2		4.008		1.478		7.734		0.172		1.336
conv5_3		2.325		1.724		13.335		0.339		0.055
scale factor:   0.075

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		25.086		1.236		1.236		0.549		0.194
conv1_2		34.234		0.733		0.906		0.212		0.119
conv2_1		29.387		1.165		1.055		0.231		0.061
conv2_2		34.185		0.860		0.907		0.118		0.037
conv3_1		27.895		1.225		1.111		0.134		0.038
conv3_2		28.723		0.971		1.079		0.101		0.030
conv3_3		27.010		1.063		1.148		0.120		0.031
conv4_1		21.086		1.281		1.470		0.124		0.038
conv4_2		14.764		1.428		2.100		0.115		0.057
conv4_3		7.611		1.940		4.073		0.178		0.187
conv5_1		5.922		1.285		5.235		0.181		0.452
conv5_2		4.008		1.478		7.734		0.171		1.336
conv5_3		2.325		1.724		13.335		0.340		0.057
scale factor:   0.075

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		25.086		1.236		1.236		0.549		0.200
conv1_2		34.234		0.733		0.906		0.212		0.135
conv2_1		29.387		1.165		1.055		0.231		0.063
conv2_2		34.185		0.860		0.907		0.119		0.040
conv3_1		27.895		1.225		1.111		0.135		0.040
conv3_2		28.723		0.971		1.079		0.101		0.035
conv3_3		27.010		1.063		1.148		0.120		0.033
conv4_1		21.086		1.281		1.470		0.124		0.039
conv4_2		14.764		1.428		2.100		0.115		0.057
conv4_3		7.611		1.940		4.073		0.176		0.187
conv5_1		5.922		1.285		5.235		0.182		0.454
conv5_2		4.008		1.478		7.734		0.172		1.336
conv5_3		2.325		1.724		13.335		0.339		0.055
scale factor:   0.075

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		25.086		1.236		1.236		0.549		0.200
conv1_2		34.234		0.733		0.906		0.212		0.135
conv2_1		29.387		1.165		1.055		0.231		0.063
conv2_2		34.185		0.860		0.907		0.119		0.040
conv3_1		27.895		1.225		1.111		0.135		0.040
conv3_2		28.723		0.971		1.079		0.101		0.035
conv3_3		27.010		1.063		1.148		0.120		0.033
conv4_1		21.086		1.281		1.470		0.124		0.039
conv4_2		14.764		1.428		2.100		0.115		0.057
conv4_3		7.611		1.940		4.073		0.176		0.187
conv5_1		5.922		1.285		5.235		0.182		0.454
conv5_2		4.008		1.478		7.734		0.172		1.336
conv5_3		2.325		1.724		13.335		0.339		0.055
scale factor:   0.075

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		24.515		1.265		1.265		0.562		0.256
conv1_2		34.356		0.714		0.902		0.214		0.181
conv2_1		29.020		1.184		1.068		0.254		0.090
conv2_2		34.836		0.833		0.890		0.119		0.046
conv3_1		25.859		1.347		1.199		0.150		0.055
conv3_2		25.457		1.016		1.218		0.115		0.045
conv3_3		22.740		1.120		1.363		0.134		0.055
conv4_1		17.004		1.337		1.823		0.146		0.064
conv4_2		11.090		1.533		2.795		0.147		0.100
conv4_3		5.459		2.032		5.679		0.194		0.282
conv5_1		3.951		1.382		7.846		0.212		0.926
conv5_2		2.598		1.521		11.933		0.208		2.520
conv5_3		1.494		1.739		20.746		0.418		0.166
scale factor:   0.0481935483871

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		29.734		1.043		1.043		0.565		0.102
conv1_2		40.102		0.741		0.773		0.161		0.042
conv2_1		32.129		1.248		0.965		0.200		0.062
conv2_2		37.109		0.866		0.835		0.111		0.037
conv3_1		23.707		1.565		1.308		0.155		0.043
conv3_2		20.935		1.132		1.481		0.100		0.048
conv3_3		17.410		1.202		1.781		0.107		0.104
conv4_1		13.241		1.315		2.341		0.104		0.095
conv4_2		10.541		1.256		2.941		0.074		0.130
conv4_3		6.714		1.570		4.617		0.104		0.278
conv5_1		5.649		1.189		5.488		0.089		0.648
conv5_2		4.179		1.352		7.418		0.134		1.014
conv5_3		2.582		1.619		12.006		0.167		0.075
scale factor:   0.0832903225806

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		29.734		1.043		1.043		0.565		0.102
conv1_2		40.102		0.741		0.773		0.161		0.042
conv2_1		32.129		1.248		0.965		0.200		0.062
conv2_2		37.109		0.866		0.835		0.111		0.037
conv3_1		23.707		1.565		1.308		0.155		0.043
conv3_2		20.935		1.132		1.481		0.100		0.048
conv3_3		17.410		1.202		1.781		0.107		0.104
conv4_1		13.241		1.315		2.341		0.104		0.095
conv4_2		10.541		1.256		2.941		0.074		0.130
conv4_3		6.714		1.570		4.617		0.104		0.278
conv5_1		5.649		1.189		5.488		0.089		0.648
conv5_2		4.179		1.352		7.418		0.134		1.014
conv5_3		2.582		1.619		12.006		0.167		0.075
scale factor:   0.0832903225806

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		30.632		1.012		1.012		0.549		0.099
conv1_2		41.608		0.736		0.745		0.161		0.040
conv2_1		36.875		1.128		0.841		0.179		0.053
conv2_2		40.775		0.904		0.760		0.119		0.033
conv3_1		24.279		1.679		1.277		0.166		0.040
conv3_2		20.983		1.157		1.477		0.102		0.048
conv3_3		17.533		1.197		1.768		0.106		0.102
conv4_1		13.480		1.301		2.300		0.103		0.092
conv4_2		10.699		1.260		2.897		0.074		0.127
conv4_3		6.826		1.567		4.542		0.103		0.275
conv5_1		5.666		1.205		5.472		0.090		0.645
conv5_2		4.160		1.362		7.453		0.134		1.022
conv5_3		2.521		1.650		12.295		0.165		0.075
scale factor:   0.0813225806452

Refreshing network prototxt and caffemodel ...

Equalizing ReLU activation output ...

Layers		ReLU Alpha	Weight Gain	Bias Gain	max weight	max bias
conv1_1		35.100		0.883		0.883		0.468		0.086
conv1_2		42.256		0.831		0.734		0.177		0.038
conv2_1		36.366		1.162		0.852		0.176		0.054
conv2_2		39.094		0.930		0.793		0.112		0.032
conv3_1		21.245		1.840		1.459		0.176		0.044
conv3_2		18.107		1.173		1.712		0.102		0.055
conv3_3		15.488		1.169		2.001		0.102		0.111
conv4_1		12.231		1.266		2.535		0.101		0.102
conv4_2		9.914		1.234		3.127		0.072		0.138
conv4_3		6.272		1.581		4.942		0.108		0.303
conv5_1		5.701		1.100		5.437		0.080		0.654
conv5_2		4.347		1.311		7.131		0.132		0.985
conv5_3		2.741		1.586		11.309		0.153		0.089
scale factor:   0.0884193548387

Refreshing network prototxt and caffemodel ...

